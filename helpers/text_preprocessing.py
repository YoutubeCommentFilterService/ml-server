from math import floor
import pandas as pd
import unicodedata
from typing import Union
import re

pattern_spacer = '=!?@'
space_pattern = re.compile(r'[ê°€-í£ã„±-ã…ã…-ã…£a-zA-Z0-9:]+[\s!?@.,â¤]*')
pattern = re.compile(rf"[{pattern_spacer}]*(\w)([{pattern_spacer}\s.,â¤]+)(\w)")

def normalize_unicode_text(text: str) -> str:
    unicode_single_hangul_dict = {
        'á„€': 'ã„±', 'á„‚': 'ã„´', 'á„ƒ': 'ã„·', 'á„…': 'ã„¹', 'á„†': 'ã…', 'á„‡': 'ã…‚', 
        'á„‰': 'ã……', 'á„‹': 'ã…‡', 'á„Œ': 'ã…ˆ', 'á„': 'ã…Š', 'á„': 'ã…‹', 'á„': 'ã…Œ', 
        'á„‘': 'ã…', 'á„’': 'ã…', 'á„': 'ã…‰', 'á„„': 'ã„¸', 'á„': 'ã„²', 'á„Š': 'ã…†', 
        'á…¡': 'ã…', 'á…£': 'ã…‘', 'á…¥': 'ã…“', 'á…§': 'ã…•', 'á…©': 'ã…—', 'á…­': 'ã…›', 
        'á…®': 'ã…œ', 'á…²': 'ã… ', 'á…³': 'ã…¡', 'á…µ': 'ã…£', 'á…¢': 'ã…', 'á…¦': 'ã…”', 
        'á…´': 'ã…¢', 'á†ª': 'ã„±ã……', 'á†¬': 'ã„´ã…ˆ', 'á†­': 'ã„´ã…', 'á†²': 'ã„¹ã…‚', 
        'á†°': 'ã„¹ã„±', 'á†³': 'ã„¹ã……', 'á†±': 'ã„¹ã…', 'á„š': 'ã„¹ã…', 'á†´': 'ã„¹ã…Œ', 
        'á†µ': 'ã„¹ã…', 'á„¡': 'ã…‚ã……', 'á„ˆ': 'ã…‚ã…‚',
        'ğ¨›': 'ã…‹', 'ãƒ²': 'ã…‹'
    }
    
    normalized = unicodedata.normalize("NFKC", text)
    normalized = ''.join(ch for ch in normalized if not unicodedata.combining(ch))
    normalized = ''.join(ch for ch in normalized if not ('\u4E00' <= ch <= '\u9FFF'))

    return ''.join(unicode_single_hangul_dict.get(ch, ch) for ch in normalized)

def normalize_korify(text: str):
    CHO = 'ã„±ã„²ã„´ã„·ã„¸ã„¹ã…ã…‚ã…ƒã……ã…†ã…‡ã…ˆã…‰ã…Šã…‹ã…Œã…ã…'
    JUNG = 'ã…ã…ã…‘ã…’ã…“ã…”ã…•ã…–ã…—ã…˜ã…™ã…šã…›ã…œã…ã…ã…Ÿã… ã…¡ã…¢ã…£'
    JUNG_MAPPING = {
        'H': 'ã…', 'I': 'ã…£', 'l': 'ã…£'
    }
    def combine_jamos(cho, jung):
        cho_idx = CHO.find(cho)
        jung_char = JUNG_MAPPING.get(jung)
        if cho_idx == -1 or not jung_char or jung_char not in JUNG:
            return cho + jung  # ì¡°í•© ë¶ˆê°€í•œ ê±´ ê·¸ëŒ€ë¡œ
        jung_idx = JUNG.find(jung_char)
        return chr(0xAC00 + cho_idx * 588 + jung_idx * 28)
    
    return re.sub(r'([ã„±-ã…])[ ,\\]*([A-Za-z])', lambda m: combine_jamos(m.group(1), m.group(2)), text)

def normalize_tlettak_font(text: str, 
                           space_pattern: Union[str, re.Pattern] = r'\s*[\wê°€-í£ã„±-ã…ã…-ã…£!?%\^\(\)\[\]\{\}\-+=~,.\/<>;:\'"]+[\s!?@.,â¤]*', 
                           search_pattern: Union[str, re.Pattern] = r'\b([\wê°€-í£ã„±-ã…ã…-ã…£!?%\^\-+=~,.\/<>;:\'"]{1}\b)([\s!?\^@.,ã…£~â¤]+)(\b[\wê°€-í£ã„±-ã…ã…-ã…£!?%\^\-+=~,.\/<>;:\'"]{1}\b)'
                           ) -> str:
    
    space_pattern = re.compile(space_pattern) if isinstance(space_pattern, str) else space_pattern
    search_pattern = re.compile(search_pattern) if isinstance(search_pattern, str) else search_pattern

    text = re.sub(r'([\(\[\{])', r'\1 ', text)  # ì—¬ëŠ” ê´„í˜¸
    text = re.sub(r'([\)\]\}])', r' \1', text)  # ë‹«ëŠ” ê´„í˜¸

    result = []
    substr = []
    pos = 0
    length = len(text)
    
    while pos < length:
        if (search_matched := search_pattern.match(text, pos)):
            substr.extend([search_matched.group(1), search_matched.group(3)])
            pos = search_matched.end() - 1
        elif (space_matched := space_pattern.match(text, pos)):
            s_end = space_matched.end()
            result.append(''.join(substr[::2]) + text[pos:s_end].strip())
            pos = s_end
            substr.clear()
        else:   # ë‘˜ ë‹¤ ë§¤ì¹­ ì‹¤íŒ¨ì¸ ê²½ìš° ë’·ë¬¸ì¥ ì „ë¶€ë¥¼ ë¶™ì—¬ì”€
            result.append(text[pos:])
            break

    text = ' ' .join(result)
    text = re.sub(r'([\(\[\{]) | ([\)\]\}])', lambda m: m.group(1) or m.group(2), text)
    return text

# ë‹‰ë„¤ì„ ì •ì œ
def clean_nickname(df: pd.DataFrame):
    def remove_if_hyphen_and_odd_word(text):
        # group(1): ìº¡ì²˜ëœ ê·¸ë£¹
        # group(0): pattern ê·¸ ìì²´
        return re.sub(
            r'-([a-zA-Z0-9]+)(?=\s|$)', 
            lambda m: '' if len(m.group(1)) % 2 == 1 else m.group(0), 
            text
        )
    
    nickname_series = (
        df['nickname']
            .str.strip()
            # .str.lower()
            .str.replace(r'^@', '', regex=True)
            .apply(lambda x: remove_if_hyphen_and_odd_word(x) if isinstance(x, str) else x)
    )
    df['nickname'] = (
        nickname_series
            .str.replace(r'[-._]', '', regex=True)
            .str.replace(r'[^a-zA-Zê°€-í£ã„±-ã…ã…-ã…£0-9]+', '', regex=True)
            .str.strip()
    )
    return df

# ë‹‰ë„¤ì„ ì •ê·œí™”
def normalize_nickname(df: pd.DataFrame):
    df['nickname'] = (
        df['nickname']
            .str.replace(r'[1Iil]9', '19', regex=True)
            .str.replace(r'[1Iil]9(?:x|ê¸ˆ)|19ê¸ˆ', '19ê¸ˆ', regex=True)
            .str.replace(r'(?:ì•¼|ì–|ì–ƒ)(?:ë™|ë‘‰|ë©|ë¡|ë‘¥|ë“•)', 'ì•¼ë™', regex=True)
            .str.replace(r'ì–ƒ(?:ì˜¹|ìš©|ì—‰|ì˜|ì›…|ìœµ)', 'ì•¼ë™', regex=True)
            .str.replace(r'[ã…‡oO0]F', 'ì•¼', regex=True)
            .str.replace(r'(?:ì±„|ì²´|ì±¼|ì³¬)(?:ë„|ë…ˆ|ë†€|ë‡°|ëˆŒ|ë‰¼)', 'ì±„ë„', regex=True)
            .str.replace(r'(?:ì±ˆ|ì²¸|ì²€|ì³°)(?:ì–¼|ì—´|ì˜¬|ìšœ|ìš¸|ìœ¨)', 'ì±„ë„', regex=True)
            .str.replace(r'(?:í”„|í‘¸)(?:ì‚¬|ìƒ¤)', 'í”„ì‚¬', regex=True)
            .str.replace(r'ì¹´g', 'ì¹´ì§€', regex=True)
            .str.replace(r'ê°¸ì…', 'ê°€ì…', regex=True)
            .str.replace(r'(?:ì˜¨|On|on|ON)(?:íŒ¬|Fan|fan|FAN)', 'ì˜¨íŒ¬', regex=True)
            .str.replace(r'(?:ë®¨|ë¬¸|ë¬´|ë®¤)ëŠ¬|ë®¨ì˜', 'ë¬¸ì˜', regex=True)
            .str.replace(r'(?:ëˆŒ|ë‰¼)(?:ëŸ¬|ë ¤)', 'ëˆŒëŸ¬', regex=True)
            .str.replace(r'(?:ì¿¨|ëŒ)ë¦­', 'í´ë¦­', regex=True)
            .str.replace(r'(?:ê¾¸|ë€¨)(?:ìš±|ìœ¡)|ë€©', 'ê¾¹', regex=True)
    )
    return df

def set_default_nickname(df: pd.DataFrame):
    def _change(nickname: str):
        if re.search(r'[ê°€-í£]', nickname):
            if len(nickname) < 3:
                return '[DEFAULT_NICK]'
        else:
            if len(nickname) < 5:
                return '[DEFAULT_NICK]'
        return nickname

    df['nickname'] = df['nickname'].apply(lambda x: _change(x) if isinstance(x, str) else x)
    return df

# í…ìŠ¤íŠ¸ ê¸°ë°˜ ì´ëª¨ì§€ ì œê±°
def clean_text_emojis(df: pd.DataFrame):
    with open('../tokens/emojis.txt', 'r', encoding='utf-8') as f:
        emojis = [line.strip() for line in f.readlines()]
    emoji_pattern = '|'.join(map(re.escape, emojis))
    df['comment'] = df['comment'].str.replace(emoji_pattern, '[TEXT_EMOJI]', regex=True)
    return df

# ìœ ë‹ˆì½”ë“œ ì •ê·œí™”
def normalize_unicode(df: pd.DataFrame):
    df['comment'] = (
        df['comment']
            .str.replace(r'[\u0020\u200b\u2002\u2003\u2007\u2008\u200c\u200d]+', ' ', regex=True)
            .str.replace(r'[\U0001F3FB-\U0001F3FF\uFE0F]', '', regex=True)
            .str.replace(r'\*+', '', regex=True)
            .apply(lambda x: replace_unicode_punctuation(x) if isinstance(x, str) else x)
            .apply(lambda x: normalize_unicode_text(x) if isinstance(x, str) else x)
            .apply(lambda x: normalize_korify(x) if isinstance(x, str) else x)
    )
    df['comment'] = df['comment'].str.replace(r'(?<!\d)(.)\1{2,}', r'\1\1', regex=True)
    return df

# ìœ ë‹ˆì½”ë“œ ë¬¸ì¥ë¶€í˜¸ ë³€í™˜
def replace_unicode_punctuation(text: str) -> str:
    unicode_punctuation_map = {
        'Â¡': '!', 'ï¼': '!',
        'Â¿': '?', 'ï¼Ÿ': '?',
        'â€˜': "'", 'â€™': "'", 'ï¼‡': "'",
        'â€œ': '"', 'â€': '"', 'ï¼‚': '"',
        'ã†': '.', 'Â·': '.', 'ãƒ»': '.', 'â€¢': '.', 'ï¼': '.',
        'ï¼Œ': ',',
        'á†¢': '..', 'â€¦': '...',
        'ï¼š': ':', 'ï¼›': ';',
        'ï¼ˆ': '(', 'ï¼‰': ')',
        'â€': '-', 'â€‘': '-', 'â€’': '-', 'â€“': '-', 'â€”': '-', 'â€•': '-',
    }
    return ''.join(unicode_punctuation_map.get(ch, ch) for ch in text)

def replace_special_tokens(df: pd.DataFrame, emoji_path: str):
    with open(emoji_path, 'r', encoding='utf-8') as f:
        emojis = [line.strip() for line in f.readlines()]
    emoji_pattern = '|'.join(map(re.escape, emojis))
    df['comment'] = (
        df['comment']
            # url ì „ì²˜ë¦¬
            .str.replace(r'https?:\/\/(?:[a-zA-Z0-9ê°€-í£ã„±-ã…ã…-ã…£\-]+\.)+(?:[a-zA-Z0-9ê°€-í£]{2,})(?::\d+)?(?:\/[^\s]*)?', '[URL]', regex=True)
            # email ì „ì²˜ë¦¬
            .str.replace(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', '[EMAIL]', regex=True)
            # tag ì „ì²˜ë¦¬
            .str.replace(r'@+[\wê°€-í£\.\-]*', '[TAG]', regex=True)
            # í•´ì‹œíƒœê·¸ ì „ì²˜ë¦¬
            .str.replace(r'#[\wê°€-í£.ã„±-ã…ã…-ã…£-]+', '[HASH_TAG]', regex=True)
            # íƒ€ì„ìŠ¤íƒ¬í”„ ì „ì²˜ë¦¬
            .str.replace(r'\d+:[0-5]\d:[0-5]\d\s*ì´ˆ?', '[TIMESTAMP]', regex=True)
            .str.replace(r'[0-5]?\d+:[0-5]\d\s*ì´ˆ?', '[TIMESTAMP]', regex=True)
            # ë¹„ìœ¨ ì „ì²˜ë¦¬. í™”ë©´ ë¹„ìœ¨ì´ë“  ê³¼ì‹¤ ë¹„ìœ¨ì´ë“ 
            .str.replace(r'\d+:\d+', '[RATIO]', regex=True)
            # í…ìŠ¤íŠ¸ ê¸°ë°˜ ì´ëª¨ì§€ ì „ì²˜ë¦¬
            .str.replace(emoji_pattern, '[TEXT_EMOJI]', regex=True)
            # ê°œì¶”, ì¶”ì²œ ìš”ì²­ ì „ì²˜ë¦¬
            .str.replace(r'([ğŸ‘†ğŸ‘ˆğŸ‘‡âœ‹ğŸ‘])', '[THUMB]', regex=True)
            # í™”ì‚´í‘œ ì „ì²˜ë¦¬
            .str.replace(r'([â¡â¬…â¬‡ââ†—â†˜â†–â†™â†’â†â†‘â†“â‡’â«ğŸ”™]|[\-=]+>+|<+[\-=]+)', '[ARROW]', regex=True)
            # í•˜íŠ¸ ì´ëª¨ì§€ ì „ì²˜ë¦¬
            .str.replace(r'[ğŸ’šğŸ’›ğŸ©·ğŸ©¶ğŸ’—ğŸ’–â¤ğŸ©µğŸ–¤ğŸ’˜â™¡â™¥ğŸ§¡ğŸ”¥ğŸ’•ï¸ğŸ¤ğŸ’œğŸ¤ğŸ’™]', '[HEART]', regex=True)
            # ì¶•í•˜ ì´ëª¨ì§€ ì „ì²˜ë¦¬
            .str.replace(r'ğŸ‰', '[CONGRAT]', regex=True)
            # yes, no ì´ëª¨ì§€ ì „ì²˜ë¦¬
            .str.replace(r'âŒ', '[NO]', regex=True)
            .str.replace(r'[â­•ï¸âœ…]', '[YES]', regex=True)
    )
    return df

def cleanup_formatting(df: pd.DataFrame):
    df['comment'] = (
        df['comment']
            # ì†Œìˆ«ì , 1000ë‹¨ìœ„ ë³€í™˜
            .str.replace(r'(?<=\d)\.(?=\d)', '[POINT_DOT]', regex=True)
            .str.replace(r'(?<=\d),(?=\d)', '[POINT_COM]', regex=True)
            # ë¬¸ì¥ë¶€í˜¸ ì•ì˜ ê³µë°± ì œê±° ë° ë’¤ì— ê³µë°± ì¶”ê°€
            .str.replace(r'\s*([.,?!^]+)\s*', r'\1 ', regex=True)
            # ì“¸ë°ì—†ì´ ë§ì€ ê³µë°± ì œê±°
            .str.replace(r'\s{2,}', ' ', regex=True)
            # ì†Œìˆ«ì , 1000ë‹¨ìœ„ ë³µì›
            .str.replace(r'\[POINT_DOT\]', '.', regex=True)
            .str.replace(r'\[POINT_COM\]', ',', regex=True)
    )
    return df

def replace_structed_patterns(df: pd.DataFrame):
    date_patterns = [
        r'\d{1,4}\s*ë…„(?:\s*\d{1,2}\s*ì›”)?(?:\s*\d{1,2}\s*ì¼)?',
        r'\d{1,2}\s*ì›”(?:\s*\d{1,2}\s*ì¼)?',
        r'\d{1,2}\s*ì¼'
    ]
    time_patterns = [
        r'\d{1,10}\s*ì‹œê°„?(?:\s*\d{1,2}\s*ë¶„)?(?:\s*\d{1,2}\s*ì´ˆ)?', # ì‹œ + ë¶„ + ì´ˆ
        r'\d{1,2}\s*ë¶„(?:\s*\d{1,2}\s*ì´ˆ)?', # ë¶„ + ì´ˆ
        r'\d{1,2}\s*ì´ˆ' # ì´ˆ
    ]
    float_patterns = [r'\d+\.\d+']
    number_patterns = [r'\d{1,3}(?:,\d{3})+', r'\d+']
    duration_patterns = [
        r'\[TIME(?:STAMP)?\][\s]*[~-][\s]*\[TIME(?:STAMP)?\]',
    ]

    patterns = [
        [date_patterns, '[DATE]'],
        [time_patterns, '[TIME]'],
        [float_patterns, '[FLOAT]'],
        [number_patterns, '[NUMBER]'],
        [duration_patterns, '[DURATION]']
    ]

    for [pattern, token] in patterns:
        for p in pattern:
            df['comment'] = df['comment'].str.replace(p, token, regex=True)

    return df

def replace_misc_patterns(df: pd.DataFrame):
    df['comment'] = (
        df['comment']
            .str.replace(r'\[+', '[', regex=True)
            .str.replace(r'\]+', ']', regex=True)
            .str.replace(r'[^\wê°€-í£ã„±-ã…ã…-ã…£!?%\^\(\)\[\]\{\}\-+=~,.\/<>;:\'"\s]', '', regex=True)
            .str.replace(r'(?<!\d)([a-zA-Zê°€-í£ã„±-ã…ã…-ã…£!?%\^\(\)\[\]\{\}\-_+=~,.\/<>;:\'"\s])\1{3,}', r'\1\1', regex=True)
            .str.strip()
            .fillna('[EMPTY]')
            # í•œê¸€ì + ë¶€í˜¸ + í•œê¸€ì íŒ¨í„´ ì²˜ë¦¬
            .apply(lambda x: normalize_tlettak_font(x) if isinstance(x, str) else x)
    )
    return df

def clean_duplicated_token(df: pd.DataFrame):
    tags = ['TIMESTAMP', 'URL', 'EMAIL', 'TAG', 'HASH_TAG', 'THUMB', 'ARROW', 'TEXT_EMOJI', 'HEART', 'CONGRAT']
    for tag in tags:
        pattern = r'(?:\[' + re.escape(tag) + r'\]\s*)+'
        df['comment'] = df['comment'].apply(lambda x: re.sub(pattern, f'[{tag}]', x) if isinstance(x, str) else x)
    return df

def run_text_preprocessing(df: pd.DataFrame, emoji_path: str):
    df = (
        normalize_unicode(df)
            .pipe(replace_special_tokens, emoji_path)
            .pipe(replace_structed_patterns)
            .pipe(cleanup_formatting)
            .pipe(replace_misc_patterns)
            .pipe(clean_duplicated_token)
    )
    df = (
        clean_nickname(df)
            .pipe(normalize_nickname)
            .pipe(set_default_nickname)
    )
    return df

def replace_regex_predict_data(df: pd.DataFrame):
    # prefix, subfix ì œê±°
    df['nickname'] = df['nickname']\
        .str.strip()\
        .str.replace('@', '')\
        .str.replace(r'-[a-zA-Z0-9]+(?=\s|$)', '', regex=True)
    # íŠ¹ìˆ˜ ê¸°í˜¸ ì œê±°
    df['nickname'] = df['nickname']\
        .str.replace(r'[-._]', '', regex=True)
    # ì˜ì–´, í•œê¸€, ìˆ«ìê°€ ì•„ë‹Œ ê²½ìš° ê¸°ë³¸ ë‹‰ë„¤ì„ ì²˜ë¦¬
    df['nickname'] = df['nickname']\
        .str.replace(r'[^a-zA-Zê°€-í£ã„±-ã…ã…-ã…£0-9]+', '[DEFAULT_NICK]', regex=True)
    
    with open('../tokens/emojis.txt', 'r', encoding='utf-8') as f:
        emojis = [line.strip() for line in f.readlines()]

    emoji_pattern = '|'.join(map(re.escape, emojis))
    df['comment'].str.replace(emoji_pattern, '[TEXT_EMOJI]', regex=True)
    
    # ìœ ë‹ˆì½”ë“œ ë¬¸ì¥ë¶€í˜¸ ìˆ˜ì •
    df['comment'] = df['comment']\
        .str.replace(r'[ã†Â·ãƒ»â€¢]', '.', regex=True)\
        .str.replace(r'[á†¢â€¦]+', '..', regex=True)\
        .str.replace(r'[â€˜â€™]+', "'", regex=True)\
        .str.replace(r'[â€œâ€]+', '"', regex=True)\
        .str.replace(r'[\u0020\u200b\u2002\u2003\u2007\u2008\u200c\u200d]+', ' ', regex=True)\
        .str.replace(r'[\U0001F3FB-\U0001F3FF\uFE0F]', '', regex=True)
    # ìœ ë‹ˆì½”ë“œ ê¾¸ë°ˆ ë¬¸ì(ê²°í•© ë¬¸ì) ì œê±°
    df['comment'] = df['comment'].str.replace(r'\*+', '', regex=True)
    df['comment'] = df['comment'].apply(lambda x: normalize_unicode_text(x) if isinstance(x, str) else x)
    # special token íŒŒì‹±
    df['comment'] = df['comment']\
        .str.replace(r'https?:\/\/(?:[a-zA-Z0-9-]+\.)*[a-zA-Z0-9ê°€-í£ã„±-ã…ã…-ã…£-]+\.[a-zA-Z]{2,}(?:\/[^?\s]*)?(?:\?[^\s]*)?', '[URL]', regex=True)\
        .str.replace(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', '[EMAIL]', regex=True)\
    # í•œê¸€ì + ë¶€í˜¸ + í•œê¸€ì íŒ¨í„´ ì²˜ë¦¬
    df['comment'] = df['comment'].apply(lambda x: normalize_tlettak_font(x, space_pattern, pattern) if isinstance(x, str) else x)
    # special token íŒŒì‹±
    df['comment'] = df['comment']\
        .str.replace(r'@{1,2}[A-Za-z0-9ê°€-í£\_\-\.]+', '[TAG]', regex=True)\
        .str.replace(r'#[A-Za-z0-9ã„±-ã…ã…-ã…£ê°€-í£\_\-\.]+', '[HASH_TAG]', regex=True)\
        .str.replace('Â¡', '!').str.replace('Â¿', '?')\
        .str.replace(r'([ğŸ‘‡âœ‹ğŸ‘])', '[THUMB]', regex=True)\
        .str.replace(r'([â¡â¬‡â†—â†˜â†–â†™â«ğŸ”™â†’â†â†‘â†“â‡’]|[\-\=]+>|<[\-\=]+)', '[ARROW]', regex=True)\
        .str.replace(r'[ğŸ’šğŸ’›ğŸ©·ğŸ©¶ğŸ’—ğŸ’–â¤ğŸ©µğŸ–¤ğŸ’˜â™¡â™¥ğŸ§¡ğŸ”¥ğŸ’•ï¸ğŸ¤ğŸ’œğŸ¤ğŸ’™]', '[HEART]', regex=True)\
        .str.replace(r'ğŸ‰', '[CONGRAT]', regex=True)
    # ì“¸ë°ì—†ì´ ë§ì€ ë¬¸ì¥ë¶€í˜¸ ì œê±°
    df['comment'] = df['comment']\
        .str.replace(r'([^\s])[.,](?=\S)', r'\1', regex=True)\
        .str.replace(r'([.,?!^]+)', r' \1 ', regex=True)\
        .str.replace(r'\s+([.,?!^]+)', r'\1', regex=True)\
        .str.replace(r'\s{2,}', ' ', regex=True)
    # timestamp ì²˜ë¦¬
    to_replace = '[TIMESTAMP]'
    df['comment'] = df['comment']\
        .str.replace(r'\d+:(?:\d+:?)?\d+', to_replace, regex=True)
    # ë°ˆ ì²˜ë¦¬
    # df['comment'] = df['comment']\
    #     .str.replace(r'(?i)chill', 'ì¹ ', regex=True)
    # í•œê¸€, ì˜ì–´ê°€ ì•„ë‹Œ ê²½ìš° ì²˜ë¦¬
    df['comment'] = df['comment']\
        .str.replace(r'[^a-zA-Z0-9ê°€-í£ã„±-ã…ã…-ã…£â™¡â™¥\!\?\@\#\$\%\^\&\*\(\)\-\_\=\+\\\~\,\.\/\<\>\[\]\{\}\;\:\'\"\s]', '', regex=True)
    # 2ê°œ ì´ìƒ ì—°ì†ëœ ë¬¸ì ì²˜ë¦¬
    df['comment'] = df['comment']\
        .str.replace(r'(.)\1{2,}', r'\1\1', regex=True)
    # ë¹ˆ ë¬¸ìì—´ì˜ ê²½ìš° empty ì²˜ë¦¬
    df['comment'] = df['comment'].str.strip()
    df['comment'] = df['comment'].fillna('[EMPTY]')

    return df

if __name__ == "__main__":
    df = pd.read_csv('../model/dataset.csv', encoding='utf-8')
    df['comment'] = df['comment'].map(lambda x: x.replace('\\', ',') if isinstance(x, str) else x)
    df['comment'] = df['comment'].str.strip()

    origin_logic_df = df.copy()
    updated_logic_df = df.copy()

    replace_regex_predict_data(origin_logic_df)
    run_text_preprocessing(updated_logic_df)

    df['comment'] = df['comment'].map(lambda x: x.replace(',', '\\') if isinstance(x, str) else x)
    origin_logic_df['comment'] = origin_logic_df['comment'].map(lambda x: x.replace(',', '\\') if isinstance(x, str) else x)
    updated_logic_df['comment'] = updated_logic_df['comment'].map(lambda x: x.replace(',', '\\') if isinstance(x, str) else x)

    comparison_origin_logic = df.compare(origin_logic_df)
    comparison_updated_logic = df.compare(updated_logic_df)
    comparison_logic = origin_logic_df.compare(updated_logic_df)
    
    with pd.ExcelWriter('comparition_results.xlsx') as writer:
        comparison_origin_logic.to_excel(writer, sheet_name="origin_logic")
        comparison_updated_logic.to_excel(writer, sheet_name="updated_logic")
        comparison_logic.to_excel(writer, sheet_name="logic_comp")

    from openpyxl import load_workbook
    wb = load_workbook('comparition_results.xlsx')

    ws_origin_logic = wb['origin_logic']
    ws_updated_logic = wb['updated_logic']
    ws_logic_comp = wb['logic_comp']

    base_width = 30

    for idx, column in enumerate(['B', 'C', 'D']):
        ws_origin_logic.column_dimensions[column].width = base_width * (1 if column in ['B', 'C'] else 5)
        ws_updated_logic.column_dimensions[column].width = base_width * (1 if column in ['B', 'C'] else 5)
        ws_logic_comp.column_dimensions[column].width = base_width * (1 if column in ['B', 'C'] else 5)

    # ìˆ˜ì •ëœ Excel íŒŒì¼ ì €ì¥
    wb.save('comparition_results_with_custom_width.xlsx')