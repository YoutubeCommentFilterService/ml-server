import pandas as pd
import unicodedata
from typing import Union
import re

def normalize_unicode_text(text: str) -> str:
    unicode_single_hangul_dict = {
        'á„€': 'ã„±', 'á„‚': 'ã„´', 'á„ƒ': 'ã„·', 'á„…': 'ã„¹', 'á„†': 'ã…', 'á„‡': 'ã…‚', 
        'á„‰': 'ã……', 'á„‹': 'ã…‡', 'á„Œ': 'ã…ˆ', 'á„': 'ã…Š', 'á„': 'ã…‹', 'á„': 'ã…Œ', 
        'á„‘': 'ã…', 'á„’': 'ã…', 'á„': 'ã…‰', 'á„„': 'ã„¸', 'á„': 'ã„²', 'á„Š': 'ã…†', 
        'á…¡': 'ã…', 'á…£': 'ã…‘', 'á…¥': 'ã…“', 'á…§': 'ã…•', 'á…©': 'ã…—', 'á…­': 'ã…›', 
        'á…®': 'ã…œ', 'á…²': 'ã… ', 'á…³': 'ã…¡', 'á…µ': 'ã…£', 'á…¢': 'ã…', 'á…¦': 'ã…”', 
        'á…´': 'ã…¢', 'á†ª': 'ã„±ã……', 'á†¬': 'ã„´ã…ˆ', 'á†­': 'ã„´ã…', 'á†²': 'ã„¹ã…‚', 
        'á†°': 'ã„¹ã„±', 'á†³': 'ã„¹ã……', 'á†±': 'ã„¹ã…', 'á„š': 'ã„¹ã…', 'á†´': 'ã„¹ã…Œ', 
        'á†µ': 'ã„¹ã…', 'á„¡': 'ã…‚ã……', 'á„ˆ': 'ã…‚ã…‚',
        'ğ¨›': 'ã…‹', 'ãƒ²': 'ã…‹'
    }
    visual_map = {
        'a': '[á´€ğ—®ğ˜¢ğ™–ğ“ªÎ±Ğ°ï¼¡ï½ğ–ºğ“]',
        'b': '[áµ‡ğ’·ğ—¯ğ™—ğ“«ğ–‡Ê™ï¼¢ï½‚ğ‘]',
        'c': '[â…½á´„Ï²â²¥Ï²â²¤Â¢åŒšğ°½áŸâ…­â„‚â…½ï¼£âˆï½ƒğ–¼ğ‘ğ’¸ğ“¬ê“]',
        'd': '[ğ–½ğ‘‘â…¾â…†ï½„ğğ—±ğ™™ğ’…ğ’Ÿğ”¡ğ••]',
        'e': '[ï½…ğğ—²ğ™šğ‘’ğ’†ğ“®ğ–¾â„®ğ”¢ğ•–ğ•°ĞµÎµ]',
        'f': '[ğ’‡ğ’»ğ“¯ğ–¿ğ•—ğŸğ—³ğ™›ï½†]',
        'g': '[ğ—€ğ“°ğ™œğ ğ‘”ğ’ˆğ“°ğ–Œï½‡]',
        'h': '[ğ¡ğ—µğ™ğ‘¯ğ’‰ğ“±ğ–ï½ˆ]',
        'i': '[ğ¢ğ—¶ğ™ğ‘–ğ’Šğ“²ğ–ï½‰]',
        'j': '[ğ£ğ—·ğ™Ÿğ‘—ğ’‹ğ“³ğ–ï½Š]',
        'k': '[ğ¤ğ—¸ğ™ ğ‘˜ğ’Œğ“´ğ–ï½‹]',
        'l': '[ğ¥ğ—¹ğ™¡ğ‘™ğ’ğ“µğ–‘â…¼ï½Œ]',
        'm': '[â…¿ğ—ºğ™¢ğ‘šğ’ğ“¶ğ–’ğ•ï½â²˜ÎœĞ¼]',
        'n': '[ğ—»ğ™£ğ‘›ğ’ğ“·ğ–“ï½ğ—‡ğ§]',
        'o': '[Î¿Ğï¼¯ã€‡ï½ğ‘œğ—ˆğ—¼ğ™¤ğ“ğ“¸ğ–”â²Ÿâ“âµ”ê‹]',
        'p': '[ğ©ğ—½ğ™¥ğ‘ğ’‘ğ“¹ğ–•ï½ğ•¡ÏÑ€]',
        'q': '[ğªğ—¾ğ™¦ğ‘ğ’’ğ“ºğ––ï½‘]',
        'r': '[ğ«ğ—¿ğ™§ğ‘Ÿğ’“ğ“»ğ–—ï½’ğ•£êƒ]',
        's': '[ğ¬ğ—¿ğ™¨ğ‘ ğ’”ğ“¼ğ–˜ï½“ğ•¤êœ±]',
        't': '[ğ­ğ—ğ™©ğ‘¡ğ’•ğ“½ğ–™ï½”ğ•¥ê‡]',
        'u': '[ğ®ğ—ğ™ªğ‘¢ğ’–ğ“¾ğ–šï½•]',
        'v': '[ğ¯ğ—ğ™«ğ‘£ğ’—ğ“¿ğ–›ï½–]',
        'w': '[ğ°ğ—ğ™¬ğ‘¤ğ’˜ğ”€ğ–œï½—]',
        'x': '[ğ±ğ—‘ğ™­ğ‘¥ğ’™ğ“ğ–ï½˜ğ•©Ñ…Ã—]',
        'y': '[ğ²ğ—’ğ™®ğ‘¦ğ’šğ“ğ–ï½™ğ•ªÑƒ]',
        'z': '[ğ³ğ—“ğ™¯ğ‘§ğ’›ğ”ƒğ–Ÿï½šğ•«á´¢]',
    }
    
    normalized = unicodedata.normalize("NFKC", text)
    normalized = ''.join(ch for ch in normalized if not unicodedata.combining(ch))
    for char, pattern in visual_map.items():
        normalized = re.sub(pattern, char, normalized)
    normalized = re.sub(r'[cC][o0O][mM]', 'com', normalized)
    normalized = ''.join(ch for ch in normalized if not ('\u4E00' <= ch <= '\u9FFF'))

    return ''.join(unicode_single_hangul_dict.get(ch, ch) for ch in normalized)

def normalize_korify(text: str):
    CHO = 'ã„±ã„²ã„´ã„·ã„¸ã„¹ã…ã…‚ã…ƒã……ã…†ã…‡ã…ˆã…‰ã…Šã…‹ã…Œã…ã…'
    JUNG = 'ã…ã…ã…‘ã…’ã…“ã…”ã…•ã…–ã…—ã…˜ã…™ã…šã…›ã…œã…ã…ã…Ÿã… ã…¡ã…¢ã…£'
    JUNG_MAPPING = {
        'H': 'ã…', 'I': 'ã…£', 'l': 'ã…£'
    }
    def combine_jamos(cho, jung):
        cho_idx = CHO.find(cho)
        jung_char = JUNG_MAPPING.get(jung)
        if cho_idx == -1 or not jung_char or jung_char not in JUNG:
            return cho + jung  # ì¡°í•© ë¶ˆê°€í•œ ê±´ ê·¸ëŒ€ë¡œ
        jung_idx = JUNG.find(jung_char)
        return chr(0xAC00 + cho_idx * 588 + jung_idx * 28)
    
    return re.sub(r'([ã„±-ã…])[ ,\\]*([A-Za-z])', lambda m: combine_jamos(m.group(1), m.group(2)), text)

def normalize_tlettak_font(text: str, 
                           space_pattern: Union[str, re.Pattern] = r'\s*[\wê°€-í£ã„±-ã…ã…-ã…£!?%\^\(\)\[\]\{\}\-+=~,.\/<>;:\'"]+[\s!?@.,â¤]*', 
                           search_pattern: Union[str, re.Pattern] = r'\b([\wê°€-í£ã„±-ã…ã…-ã…£!?%\&\^\-+=~,.\/<>;:\'"]{1}\b)([\s!?\^@.,ã…£~â¤]+)(\b[\wê°€-í£ã„±-ã…ã…-ã…£!?%\^\-+=~,.\/<>;:\'"]{1}\b)'
                           ) -> str:
    
    space_pattern = re.compile(space_pattern) if isinstance(space_pattern, str) else space_pattern
    search_pattern = re.compile(search_pattern) if isinstance(search_pattern, str) else search_pattern

    text = re.sub(r'([\(\[\{])', r'\1 ', text)  # ì—¬ëŠ” ê´„í˜¸
    text = re.sub(r'([\)\]\}])', r' \1', text)  # ë‹«ëŠ” ê´„í˜¸

    result = []
    substr = []
    pos = 0
    length = len(text)
    
    while pos < length:
        if (search_matched := search_pattern.match(text, pos)):
            substr.extend([search_matched.group(1), search_matched.group(3)])
            pos = search_matched.end() - 1
        elif (space_matched := space_pattern.match(text, pos)):
            s_end = space_matched.end()
            result.append(''.join(substr[::2]) + text[pos:s_end].strip())
            pos = s_end
            substr.clear()
        else:   # ë‘˜ ë‹¤ ë§¤ì¹­ ì‹¤íŒ¨ì¸ ê²½ìš° ë’·ë¬¸ì¥ ì „ë¶€ë¥¼ ë¶™ì—¬ì”€
            result.append(text[pos:])
            break

    text = ' ' .join(result)
    text = re.sub(r'([\(\[\{]) | ([\)\]\}])', lambda m: m.group(1) or m.group(2), text)
    return text

# ìœ ë‹ˆì½”ë“œ ì •ê·œí™”
def _normalize_unicode(df: pd.DataFrame):
    df['comment'] = (
        df['comment'] # \u2640\u2642\u2695\u2696\u2708\u2764
            .str.replace(r'[\u0020\u200b\u2002\u2003\u2007\u2008\u200c\u200d]+', ' ', regex=True)
            .str.replace(r'[\U0001F3FB-\U0001F3FF\uFE0F]', '', regex=True)
            .str.replace(r'\*+', '', regex=True)
            .str.replace('9ê¸€', 'êµ¬ê¸€')
            .apply(lambda x: replace_unicode_punctuation(x) if isinstance(x, str) else x)
            .apply(lambda x: normalize_unicode_text(x) if isinstance(x, str) else x)
            .apply(lambda x: normalize_korify(x) if isinstance(x, str) else x)
    )
    df['comment'] = df['comment'].str.replace(r'(?<!\d)(.)\1{2,}', r'\1\1', regex=True)
    return df

# ìœ ë‹ˆì½”ë“œ ë¬¸ì¥ë¶€í˜¸ ë³€í™˜
def replace_unicode_punctuation(text: str) -> str:
    unicode_punctuation_map = {
        '!': r'[Â¡ï¼â—]',
        '!?': r'[â‰]',
        '?': r'[Â¿ï¼Ÿ]',
        "'": r'[â€˜â€™ï¼‡]',
        '"': r'[â€œâ€ï¼‚]',
        '.': r'[ã†Â·ãƒ»â€¢ï¼]',
        ',': r'[ï¼Œ]',
        '..': r'[á†¢]',
        '...': r'[â€¦]',
        ':': r'[ï¼š]',
        ';': r'[ï¼›]',
        '(': r'[ï¼ˆ]',
        ')': r'[ï¼‰]',
        '-': r'[â€â€‘â€’â€“â€”â€•]',
    }
    for key, pattern in unicode_punctuation_map.items():
        text = re.sub(pattern, key, text)
    return text

def _replace_special_tokens(df: pd.DataFrame, emoji_path: str):
    def _replace_emoji(text: str):
        def _shrink_single_tags(emoji_str: str):
            return r'(?:[' + ''.join(re.escape(c) for c in emoji_str) + r']\s*)+'
        def _shrink_combined_tags(capture_patterns: str):
            return r'(?:' + '|'.join(capture_patterns) + r'\s*)+'
        text = re.sub(_shrink_single_tags('ğŸ˜°ğŸ˜¨ğŸ˜¥ğŸ˜“ğŸ˜–ğŸ˜©ğŸ˜¬ğŸ¥µ'), '[FACE_NERVOUS]', text)
        text = re.sub(_shrink_single_tags('ğŸ˜ğŸ˜'), '[FACE_COOL]', text)
        text = re.sub(_shrink_single_tags('ğŸ¤’ğŸ¤•ğŸ¤¢ğŸ¤®ğŸ¤§ğŸ˜·'), '[FACE_SICK]', text)
        text = re.sub(_shrink_single_tags('ğŸ˜¬ğŸ˜³ğŸ˜¶'), '[FACE_AWKWARD]', text)
        text = re.sub(_shrink_single_tags('ğŸ¤”ğŸ§ğŸ¤·ğŸ¤·â€â™‚ï¸ğŸ¤·â€â™€'), '[FACE_CURIOUS]', text)
        text = re.sub(_shrink_single_tags('ğŸ˜®ğŸ˜²ğŸ«¢ğŸ˜³ğŸ˜¯ğŸ˜±ğŸ™€'), '[FACE_SURPRISE]', text)
        text = re.sub(_shrink_single_tags('ğŸ˜ ğŸ˜¡ğŸ’¢ğŸ‘¿ğŸ˜¤'), '[FACE_ANGRY]', text)
        text = re.sub(_shrink_single_tags('ğŸ˜¢ğŸ˜¥ğŸ¥²ğŸ˜­ğŸ˜ğŸ˜”ğŸ˜ŸğŸ¥ºğŸ¥¹ğŸ˜¿'), '[FACE_SAD]', text)
        text = re.sub(_shrink_single_tags('ğŸ˜‚ğŸ¤£ğŸ¤­ğŸ˜¹'), '[FACE_LAUGH]', text)
        text = re.sub(_shrink_single_tags('ğŸ˜€ğŸ˜ƒğŸ˜„ğŸ˜ğŸ˜†ğŸ˜ŠğŸ™‚ğŸ¤—ğŸ¤©ğŸ¤¤ğŸ¤“ğŸ™ƒ'), '[FACE_SMILE]', text)
        text = re.sub(_shrink_single_tags('ğŸ˜•ğŸ¤¨ğŸ˜…'), '[FACE_SARCASM]', text)
        text = re.sub(_shrink_single_tags('ğŸ™ğŸ•Š'), '[PRAY]', text)
        text = re.sub(_shrink_single_tags('ğŸ’ğŸ’•ğŸ’•ğŸ’—ğŸ’˜ğŸ’–â¤â¤ğŸ§¡ğŸ’›ğŸ’šğŸ’™ğŸ’œğŸ–¤ğŸ¤ğŸ¤ğŸ’ŸğŸ©·ğŸ©µğŸ©¶â£ğŸ’ğŸ˜˜ğŸ¥°ğŸ˜ğŸ˜šğŸ˜™â™¡â™¥'), '[HEART]', text)
        text = re.sub(r'(?i)' + _shrink_combined_tags([r'l(?:o|\[HEART\])?ve', r'ì‚¬ë‘í•´(?:ìš”)?\b', r'\bì‚¬ë‘í•´(?:ìš”)?', r'ì¢‹ì•„í•´?ìš”', r'ì¢‹ì•„í•´ìš”?']), '[HEART]', text)
        text = re.sub(_shrink_single_tags('ğŸ‰ğŸ¥³ğŸŠğŸ‘ğŸ¥‚'), '[CONGRAT]', text)
        text = re.sub(_shrink_single_tags('âŒ'), '[NO]', text)
        text = re.sub(_shrink_single_tags('â­•ï¸âœ…'), '[YES]', text)
        text = re.sub(_shrink_single_tags('âœ‹ğŸ‘ğŸ™‹'), '[THUMB]', text)
        text = re.sub(_shrink_single_tags('â¡â¬…â¬‡ââ†—â†˜â†–â†™â†’â†â†‘â†“â‡’â«ğŸ”™ğŸ‘†ğŸ‘ˆğŸ‘‡'), '[ARROW]', text)
        text = re.sub(_shrink_combined_tags([r'[\-=]+>+', r'<+[\-=]+']), '[ARROW]', text)
        return text
    with open(emoji_path, 'r', encoding='utf-8') as f:
        emojis = [line.strip() for line in f.readlines()]
    emoji_pattern = '|'.join(map(re.escape, emojis))
    df['comment'] = (
        df['comment']
            # url ì „ì²˜ë¦¬
            .str.replace(r'https?:\/\/(?:[a-zA-Z0-9ê°€-í£ã„±-ã…ã…-ã…£\-]+\.)+(?:[a-zA-Z0-9ê°€-í£]{2,})(?::\d+)?(?:\/[^\s]*)?', '[URL]', regex=True)
            # email ì „ì²˜ë¦¬
            .str.replace(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', '[EMAIL]', regex=True)
            # tag ì „ì²˜ë¦¬
            .str.replace(r'@+[\wê°€-í£\.\-]+', '[TAG]', regex=True)
            # í•´ì‹œíƒœê·¸ ì „ì²˜ë¦¬
            .str.replace(r'#[\wê°€-í£.ã„±-ã…ã…-ã…£-]+', '[HASH_TAG]', regex=True)
            # ì¹´ìš´íŠ¸ë‹¤ìš´, IP ì „ì²˜ë¦¬
            .str.replace(r'(?:\d+\s*[.,]+){4,}', '[STEP]', regex=True)
            .str.replace(r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}', '[IP]', regex=True)
            # íƒ€ì„ìŠ¤íƒ¬í”„ ì „ì²˜ë¦¬
            .str.replace(r'\d+:[0-5]\d:[0-5]\d(?:\s*ì´ˆ)?', '[TIMESTAMP]', regex=True)
            .str.replace(r'\d+:[0-5]\d(?:\s*ì´ˆ)?', '[TIMESTAMP]', regex=True)
            # ë¹„ìœ¨ ì „ì²˜ë¦¬. í™”ë©´ ë¹„ìœ¨ì´ë“  ê³¼ì‹¤ ë¹„ìœ¨ì´ë“ 
            .str.replace(r'\d+:\d+', '[RATIO]', regex=True)
            # í…ìŠ¤íŠ¸ ê¸°ë°˜ ì´ëª¨ì§€ ì „ì²˜ë¦¬
            .str.replace(emoji_pattern, '[TEXT_EMOJI]', regex=True)
            # ê°œì¶”, ì¶”ì²œ ìš”ì²­ ì „ì²˜ë¦¬
            .apply(lambda x: _replace_emoji(x) if isinstance(x, str) else x)
    )
    return df

def _cleanup_formatting(df: pd.DataFrame):
    df['comment'] = (
        df['comment']
            # ì†Œìˆ«ì , 1000ë‹¨ìœ„ ë³€í™˜
            .str.replace(r'(?<=\d)\.(?=\d)', '[POINT_DOT]', regex=True)
            .str.replace(r'(?<=\d),(?=\d)', '[POINT_COM]', regex=True)
            # ë¬¸ì¥ë¶€í˜¸ ì•ì˜ ê³µë°± ì œê±° ë° ë’¤ì— ê³µë°± ì¶”ê°€
            .str.replace(r'\s*([.,?!^]+)\s*', r'\1 ', regex=True)
            # ì“¸ë°ì—†ì´ ë§ì€ ê³µë°± ì œê±°
            .str.replace(r'\s{2,}', ' ', regex=True)
            # ì†Œìˆ«ì , 1000ë‹¨ìœ„ ë³µì›
            .str.replace(r'\[POINT_DOT\]', '.', regex=True)
            .str.replace(r'\[POINT_COM\]', ',', regex=True)
    )
    return df

def _replace_structed_patterns(df: pd.DataFrame):
    date_patterns = [
        r'\d{2,4}\s*\.\s*\d{1,2}\s*\.\s*\d{1,2}\s*\.?',
        r'\d{2,4}\s*\-\s*\d{1,2}\s*\-\s*\d{1,2}\s*',
        r'\d{2,4}\s*\/\s*\d{1,2}\s*\/\s*\d{1,2}\s*',
        r'\d{1,4}\s*ë…„(?:\s*\d{1,2}\s*ì›”)?(?:\s*\d{1,2}\s*ì¼)?',
        r'\d+\s*ê°œ?ì›”(?:\s*\d{1,2}\s*ì¼)?',
        r'\d+\s*(?:ì¼|ì£¼|ì¤‘ìˆœ|ë‹¬)', # 100ì¼ ìƒì¡´ê¸° ë“±ë“±ì„ ê²€ì¶œí•˜ê¸° ìœ„í•¨
        r'\[DATE]\s*ì¤‘ìˆœ',
        r'(?i)në…„',
        r'\d+\s*~\s*\[DATE\]ì°¨?',
    ]
    time_patterns = [
        r'(?:ë°¤|ë‚®|ì˜¤ì „|ì˜¤í›„)?\s*(?:\d+|ëª‡)\s*ì‹œê°„?(?:\s*(?:\d+|ëª‡)\s*ë¶„)?(?:\s*(?:\d+|ëª‡)\s*ì´ˆ)?', # ì‹œ + ë¶„ + ì´ˆ,
        r'(?:\d+|ëª‡)\s*ë¶„(?:\s*(?:\d+|ëª‡)\s*ì´ˆ)?',
        r'(?:(?:\d+\.)?\d+|ëª‡)\s*ì´ˆ',
        r'\[TIME\]ì¯¤',
    ]
    days_pattern = [
        r'(?:[\[\(][ì›”í™”ìˆ˜ëª©ê¸ˆí† ì¼](?:ìš”ì¼)?[\)\]]|[ì›”í™”ìˆ˜ëª©ê¸ˆí† ì¼]ìš”ì¼)',
    ]
    float_patterns = [
        r'\d+\.\d+',
    ]
    number_patterns = [
        r'\d{1,3}(?:,\d{3})+', 
        r'\d+',
        # ì¼ë‹¨ ë‹¨ì¼ "ë§Œ", "ì²œ" ë“±ì˜ ë‹¨ìœ„ëŠ” ë¬´ì‹œí•˜ì. ë§Œì˜¤ë°±ì› ì´ëŸ°ê±°ëŠ” ì¢€ ê±°ë¥´ê³ ì‹¶ì€ë°...
        r'(?:(?:\[NUMBER\]|ëª‡)(?:ì‹­ë§Œ|ë°±ë§Œ|ì²œë§Œ|ì‹­|ë°±|ì²œ|ë§Œ|ì–µ|ì¡°|ê²½(?!ê¸°))\s*)+',
        r'\[NUMBER\],\s*\[NUMBER\]',
        r'\[NUMBER\]\/\[NUMBER\]',
        r'[\+\-]\[NUMBER\]',
    ]
    duration_patterns = [
        r'\[TIME(?:STAMP)?\]\s*[~-]\s*\[TIME(?:STAMP)?\]',
    ]
    kda_patterns = [
        r'\d+\/\d+\/\d+',
    ]
    range_patterns = [
        r'(?:\[NUMBER\]|\[FLOAT\]|\[DATE\])\s*~\s*(?:\[NUMBER\]|\[FLOAT\]|\[DATE\])',
    ]
    percent_patterns = [
        r'(?:\[NUMBER\]|\[FLOAT\])(?:%|í¼(?:ì„¼íŠ¸)?|í”„ë¡œ)', # "4í”„ë¡œë¸Œ ì¡í˜”ë‹¤ ì—ì„œ ì˜¤ë¥˜ ìƒê¸¸ ì˜ˆì •"
    ]
    cost_patterns = [
        r'(?:\[NUMBER\]|\[RANGE\])\s*(?:ë‹¬ëŸ¬|ì½”(?!ì–´)(?:ìŠ¤íŠ¸|ì¸)?|ì›|â‚©|\$|ê³¨ë“œ)',
        r'\[COST\]\s*ëŒ€',
        r'ìˆ˜?(?:ì‹­|ë°±|ì²œ|ë§Œ)ì›',
    ]
    rank_patterns = [
        r'(?:\[NUMBER\]|\[RANGE\])\s*(?:ìœ„|ë“±|ë¹ ë”°?|ë²ˆì§¸)',
        r'(?i)\bno.\s*\[NUMBER\]',
    ]
    anniversary_patterns = [
        r'\[DATE\](?:ì£¼ë…„|ì°¨)',
    ]
    measure_patterns = [
        r'(?i)(?:\[NUMBER\]|\[FLOAT\])(?:[kmg]?[gb]|ê°œ|ì„¸íŠ¸|ì…‹|m|mm|ml|l|ë²ˆ|ê·¸ë¨|ì¤„|ì—°?ìŠ¹|ë·°|í‰|í•‘)',
    ]
    unit_patterns = [# |ì°¨
        r'(?:\[NUMBER\]|\[RANGE\])(?:íšŒì°¨|ì½”ì–´?|í˜¸ê¸°?|ë°°ì†?|ë§ˆë¦¬|ê²½ê¸°|ë ˆë²¨|ë ™|í™”|ë²ˆ|íšŒ|í¸|ì„¸ëŒ€?|ì‚´|ì¸µ|ë¶€|ì¥|íŒ|ëª…|í‚¬|í‘œ|ìˆ˜|ì„±|êµ°|ì¹¸|íŠ¸|ì¹´|ì¸)',
        r'(?i)[a-zA-Z]{1,6}\s*\[NUMBER\]\s*[a-zA-Z]{1,5}\s*[a-zA-Z]{1,5}', # pro max
        r'(?i)[a-zA-Z]{1,6}\s*\[NUMBER\]\s*[a-zA-Z]{1,5}', #  iphone 13 pro, ultra
        r'(?i)[a-zA-Z]{1,6}\s*\[NUMBER\]', # rtx3080, iphone 3070 ì•„ì´í°, ê°¤ëŸ­ì‹œ 
        r'(?i)\[NUMBER\]\s*[a-zA-Z]{1,5}', # 
        r'\[DATE\]ìƒ',
        r'\[NUMBER\]-\[NUMBER\]',
        r'\[NUMBER\]ì¹´',
    ]
    step_patterns = [
        r'\[NUMBER\]\.',
    ]

    normalize_patterns = {
        '[TIME]': [r'\[NUMBER\]\[TIME\]',],
        ' ì¡°íšŒìˆ˜': [r'\[UNIT\]ìˆ˜',],
        '[DATE]': [r'\[NUMBER\]\s*,\[DATE\]', r'\[DATE\]\[DAYS\]',],
        '[DURATION]': [r'(?:\[DATE\]|\[NUMBER\])\s*[~-]\s*\[DATE\]', r'\[NUMBER\]\s*[~-]\s*\[TIME]',],
        '[COST]': [r'\[FLOAT\]\[COST\]', r'\[NUMBER\]\[COST\]',],
        '[HEART]': [r'\[NUMBER\]\s*\[HEART\]]',],
    }

    patterns = {
        '[DAYS]': days_pattern,
        '[DATE]': date_patterns,
        '[TIME]': time_patterns,
        '[KDA]': kda_patterns,
        '[FLOAT]': float_patterns,
        '[NUMBER]': number_patterns,
        '[DURATION]': duration_patterns,
        '[RANGE]': range_patterns,
        '[PERCENT]': percent_patterns,
        '[COST]': cost_patterns,
        '[RANK]': rank_patterns,
        '[ANNIV]': anniversary_patterns,
        '[MEASURE]': measure_patterns,
        '[UNIT]': unit_patterns,
        '[STEP]': step_patterns,
    }

    for key, regexs in patterns.items():
        for regex in regexs:
            df['comment'] = df['comment'].str.replace(regex, key, regex=True)

    for key, regexs in normalize_patterns.items():
        for regex in regexs:
            df['comment'] = df['comment'].str.replace(regex, key, regex=True)
    return df

def _replace_misc_patterns(df: pd.DataFrame):
    def _fix_spam_likely_text(text: str):
        pattern = r'(?:([ê°€-í£]+)ã…£([ê°€-í£]+))+'
        while len(re.findall(pattern, text)):
            text = re.sub(pattern, r'\1\2', text)
        return text
    df['comment'] = (
        df['comment']
            .str.replace(r'\[+', '[', regex=True)
            .str.replace(r'\]+', ']', regex=True)
            .str.replace(r'[^\wê°€-í£ã„±-ã…ã…-ã…£!?%&\^()\[\]{}\-+=~,./<>;:\'"\s]', '', regex=True)
            .str.replace(r'(?<!\d)([a-zA-Zê°€-í£ã„±-ã…ã…-ã…£!?%\^\(\)\[\]\{\}\-_+=~,.\/<>;:\'"\s])\1{3,}', r'\1\1', regex=True)
            .str.strip()
            .fillna('[EMPTY]')
            # í•œê¸€ì + ë¶€í˜¸ + í•œê¸€ì íŒ¨í„´ ì²˜ë¦¬
            .apply(lambda x: normalize_tlettak_font(x) if isinstance(x, str) else x)
            .apply(lambda x: _fix_spam_likely_text(x) if isinstance(x, str) else x)
    )
    return df

def _clean_duplicated_token(df: pd.DataFrame):
    tags = [
        'ANNIV', 'ARROW', 'CONGRAT', 'COST', 'DATE', 
        'DURATION', 'EMAIL', 'EMPTY', 'FACE_ANGRY', 'FACE_AWKWARD', 
        'FACE_COOL', 'FACE_CURIOUS', 'FACE_LAUGH', 'FACE_NERVOUS',
        'FACE_SAD', 'FACE_SARCASM', 'FACE_SICK', 'FACE_SMILE',
        'FACE_SURPRISE', 'FLOAT', 'HASH_TAG', 'HEART',
        'IP', 'KDA', 'MEASURE', 'NO', 'NUMBER', 
        'PERCENT', 'PRAY', 'RANGE', 'RANK', 'RATIO', 'STEP', 'TAG',
        'TEXT_EMOJI', 'THUMB', 'TIMESTAMP', 'TIME', 'UNIT', 'URL', 'YES',
    ]
    for tag in tags:
        pattern = r'(?:\[' + re.escape(tag) + r'\]\s*)+'
        df['comment'] = df['comment'].apply(lambda x: re.sub(pattern, f'[{tag}]', x) if isinstance(x, str) else x)

    def _simplyfy_brackets(text: str):
        text = re.sub(r'[\[\(\{]+', lambda m: m.group(0)[-1], text)
        text = re.sub(r'[\]\)\}]+', lambda m: m.group(0)[-1], text)
        return text
    df['comment'] = (
        df['comment']
            .apply(lambda x: _simplyfy_brackets(x) if isinstance(x, str) else x)
    )
    return df

# ë‹‰ë„¤ì„ ì •ì œ
def _clean_nickname(df: pd.DataFrame):
    # userë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš° ì²´í¬
    def _change_if_nickname_is_default(text: str):
        # íƒœê·¸ë¡œ ë¶™ëŠ” ë§¨ ë’¤ì˜ - ì˜ ê²½ìš°, ë’¤ì— ëŒ€ë¬¸ìê°€ "ì „í˜€" ì˜¤ì§€ ì•ŠëŠ”ë‹¤.
        return re.sub(
            r'^user-([a-z0-9]+)$',
            lambda m: '[DEFAULT_NICK]' if len(m.group(1)) % 2 == 1 or len(m.group(1)) > 7 else m.group(0),
            text
        )
    
    def _change_if_starts_with_user(text: str):
        return re.sub(
            r'^user[-_.]([a-zA-Z0-9ê°€-í£-._]+)$',
            r'\1',
            text
        )
    
    def _remove_if_hyphen_and_odd_word(text: str):
        # group(1): ìº¡ì²˜ëœ ê·¸ë£¹
        # group(0): pattern ê·¸ ìì²´
        pattern = r'-([a-z0-9]+)$'
        before_text = text
        while re.search(pattern, text):
            text = re.sub(
                pattern, 
                lambda m: '' if len(m.group(1)) % 2 == 1 or len(m.group(1)) > 7 else m.group(0), 
                text
            )
            if text == before_text:
                break
            before_text = text

        return text
    
    def _normalize_nickname(text: str):
        if text == '[DEFAULT_NICK]':
            return text
        text = re.sub(r'[-._]', '', text)
        text = re.sub(r'[^a-zA-Zê°€-í£0-9]+', '', text)
        return text
    
    df['nickname'] = (
        df['nickname']
            .str.strip()
            .str.replace(r'^@', '', regex=True)
            .apply(lambda x: _change_if_nickname_is_default(x) if isinstance(x, str) else x)
            .apply(lambda x: _change_if_starts_with_user(x) if isinstance(x, str) else x)
            .apply(lambda x: _remove_if_hyphen_and_odd_word(x) if isinstance(x, str) else x)
            .apply(lambda x: _normalize_nickname(x) if isinstance(x, str) else x)
    )
    return df

# ë‹‰ë„¤ì„ ì •ê·œí™”
def _normalize_spam_nickname(df: pd.DataFrame):
    df['nickname'] = (
        df['nickname']
            .str.replace(r'(?i)[1il]9', '19', regex=True)
            .str.replace(r'(?i)[1il]9(?:x|ê¸ˆ)', '19ê¸ˆ', regex=True)
            .str.replace(r'[ã…‡oO0]F([ê°€-í£])', r'ì•¼\1', regex=True)
            .str.replace(r'(?:ì•¼|ì–|ì–ƒ)\w*(?:ë™|ë‘‰|ë©|ë¡|ë‘¥|ë“•)', 'ì•¼ë™', regex=True)
            .str.replace(r'ì–ƒ\w*(?:ì˜¹|ìš©|ì—‰|ì˜|ì›…|ìœµ)', 'ì•¼ë™', regex=True)
            .str.replace(r'(?:ì±„|ì²´|ì±¼|ì³¬)(?:ë„|ë…ˆ|ë†€|ë‡°|ëˆŒ|ë‰¼)', 'ì±„ë„', regex=True)
            .str.replace(r'(?:ì±ˆ|ì²¸|ì²€|ì³°)(?:ì–¼|ì—´|ì˜¬|ìšœ|ìš¸|ìœ¨)', 'ì±„ë„', regex=True)
            .str.replace(r'(?:í”„|í‘¸)(?:ì‚¬|ìƒ¤)', 'í”„ì‚¬', regex=True)
            .str.replace(r'(?i)ì¹´g', 'ì¹´ì§€', regex=True)
            .str.replace(r'(?i)v[1l]p', 'VIP', regex=True)
            .str.replace(r'(?i)(?:ì˜¨|on)(?:íŒ¬|fan)', 'ì˜¨íŒ¬', regex=True)
            .str.replace(r'(?:ë®¨|ë¬¸|ë¬´|ë®¤)(?:ì˜|ëŠ¬|í¬)', 'ë¬¸ì˜', regex=True)
            .str.replace(r'(?:ëˆŒ|ë‰¼)(?:ëŸ¬|ë ¤)', 'ëˆŒëŸ¬', regex=True)
            .str.replace(r'(?:ì¿¨|ëŒ)(?:ë¦­|ë§„)|í´ë§„', 'í´ë¦­', regex=True)
            .str.replace(r'(?:ê¾¸|ë€¨)(?:ìš±|ìœ¡)|ë€©', 'ê¾¹', regex=True)
            .str.replace(r'(?:ìƒ¤|ã……F)ê³ ', 'ì‚¬ê³ ', regex=True)
            .str.replace(r'ê°¸ì…', 'ê°€ì…', regex=False)
            .str.replace(r'ë±¡', 'ë°©', regex=False)
            .str.replace(r'ì¥¬ì†Œ', 'ì£¼ì†Œ', regex=False)
            .str.replace(r'ê¾¤', 'ê¼´', regex=False)
            .str.replace(r'ì¡', 'ì •', regex=False)
            .str.replace(r'ëƒ¬', 'ë‚´', regex=False)
            .str.replace(r'ë¼Œ', 'ë³€', regex=False)
    )
    return df

def _remove_isolated_english(df: pd.DataFrame):
    df['nickname'] = (
        df['nickname']
            .str.replace(r'(?<=[ê°€-í£])([a-zA-Z])(?=[ê°€-í£])(?!ì–‘)', '', regex=True)
    )
    return df

def _set_default_nickname(df: pd.DataFrame):
    def _change(nickname: str):
        if re.search(r'[ê°€-í£]', nickname):
            if len(nickname) < 3:
                return '[DEFAULT_NICK]'
        else:
            if len(nickname) < 5:
                return '[DEFAULT_NICK]'
        return nickname

    df['nickname'] = df['nickname'].apply(lambda x: _change(x) if isinstance(x, str) else x)
    return df

def run_text_preprocessing(df: pd.DataFrame, emoji_path: str):
    df = (
        _normalize_unicode(df)
            .pipe(_replace_special_tokens, emoji_path)
            .pipe(_replace_structed_patterns)
            .pipe(_cleanup_formatting)
            .pipe(_replace_misc_patterns)
            .pipe(_clean_duplicated_token)
    )
    df = (
        _clean_nickname(df)
            .pipe(_normalize_spam_nickname)
            .pipe(_remove_isolated_english)
            .pipe(_set_default_nickname)
    )
    return df

def replace_regex_predict_data(df: pd.DataFrame):
    pattern_spacer = '=!?@'
    space_pattern = re.compile(r'[ê°€-í£ã„±-ã…ã…-ã…£a-zA-Z0-9:]+[\s!?@.,â¤]*')
    pattern = re.compile(rf"[{pattern_spacer}]*(\w)([{pattern_spacer}\s.,â¤]+)(\w)")
    # prefix, subfix ì œê±°
    df['nickname'] = df['nickname']\
        .str.strip()\
        .str.replace('@', '')\
        .str.replace(r'-[a-zA-Z0-9]+(?=\s|$)', '', regex=True)
    # íŠ¹ìˆ˜ ê¸°í˜¸ ì œê±°
    df['nickname'] = df['nickname']\
        .str.replace(r'[-._]', '', regex=True)
    # ì˜ì–´, í•œê¸€, ìˆ«ìê°€ ì•„ë‹Œ ê²½ìš° ê¸°ë³¸ ë‹‰ë„¤ì„ ì²˜ë¦¬
    df['nickname'] = df['nickname']\
        .str.replace(r'[^a-zA-Zê°€-í£ã„±-ã…ã…-ã…£0-9]+', '[DEFAULT_NICK]', regex=True)
    
    with open('../tokens/emojis.txt', 'r', encoding='utf-8') as f:
        emojis = [line.strip() for line in f.readlines()]

    emoji_pattern = '|'.join(map(re.escape, emojis))
    df['comment'].str.replace(emoji_pattern, '[TEXT_EMOJI]', regex=True)
    
    # ìœ ë‹ˆì½”ë“œ ë¬¸ì¥ë¶€í˜¸ ìˆ˜ì •
    df['comment'] = df['comment']\
        .str.replace(r'[ã†Â·ãƒ»â€¢]', '.', regex=True)\
        .str.replace(r'[á†¢â€¦]+', '..', regex=True)\
        .str.replace(r'[â€˜â€™]+', "'", regex=True)\
        .str.replace(r'[â€œâ€]+', '"', regex=True)\
        .str.replace(r'[\u0020\u200b\u2002\u2003\u2007\u2008\u200c\u200d]+', ' ', regex=True)\
        .str.replace(r'[\U0001F3FB-\U0001F3FF\uFE0F]', '', regex=True)
    # ìœ ë‹ˆì½”ë“œ ê¾¸ë°ˆ ë¬¸ì(ê²°í•© ë¬¸ì) ì œê±°
    df['comment'] = df['comment'].str.replace(r'\*+', '', regex=True)
    df['comment'] = df['comment'].apply(lambda x: normalize_unicode_text(x) if isinstance(x, str) else x)
    # special token íŒŒì‹±
    df['comment'] = df['comment']\
        .str.replace(r'https?:\/\/(?:[a-zA-Z0-9-]+\.)*[a-zA-Z0-9ê°€-í£ã„±-ã…ã…-ã…£-]+\.[a-zA-Z]{2,}(?:\/[^?\s]*)?(?:\?[^\s]*)?', '[URL]', regex=True)\
        .str.replace(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', '[EMAIL]', regex=True)\
    # í•œê¸€ì + ë¶€í˜¸ + í•œê¸€ì íŒ¨í„´ ì²˜ë¦¬
    df['comment'] = df['comment'].apply(lambda x: normalize_tlettak_font(x, space_pattern, pattern) if isinstance(x, str) else x)
    # special token íŒŒì‹±
    df['comment'] = df['comment']\
        .str.replace(r'@{1,2}[A-Za-z0-9ê°€-í£\_\-\.]+', '[TAG]', regex=True)\
        .str.replace(r'#[A-Za-z0-9ã„±-ã…ã…-ã…£ê°€-í£\_\-\.]+', '[HASH_TAG]', regex=True)\
        .str.replace('Â¡', '!').str.replace('Â¿', '?')\
        .str.replace(r'([ğŸ‘‡âœ‹ğŸ‘])', '[THUMB]', regex=True)\
        .str.replace(r'([â¡â¬‡â†—â†˜â†–â†™â«ğŸ”™â†’â†â†‘â†“â‡’]|[\-\=]+>|<[\-\=]+)', '[ARROW]', regex=True)\
        .str.replace(r'[ğŸ’šğŸ’›ğŸ©·ğŸ©¶ğŸ’—ğŸ’–â¤ğŸ©µğŸ–¤ğŸ’˜â™¡â™¥ğŸ§¡ğŸ’•ï¸ğŸ¤ğŸ’œğŸ¤ğŸ’™]', '[HEART]', regex=True)\
        .str.replace(r'ğŸ‰', '[CONGRAT]', regex=True)
    # ì“¸ë°ì—†ì´ ë§ì€ ë¬¸ì¥ë¶€í˜¸ ì œê±°
    df['comment'] = df['comment']\
        .str.replace(r'([^\s])[.,](?=\S)', r'\1', regex=True)\
        .str.replace(r'([.,?!^]+)', r' \1 ', regex=True)\
        .str.replace(r'\s+([.,?!^]+)', r'\1', regex=True)\
        .str.replace(r'\s{2,}', ' ', regex=True)
    # timestamp ì²˜ë¦¬
    to_replace = '[TIMESTAMP]'
    df['comment'] = df['comment']\
        .str.replace(r'\d+:(?:\d+:?)?\d+', to_replace, regex=True)
    # ë°ˆ ì²˜ë¦¬
    # df['comment'] = df['comment']\
    #     .str.replace(r'(?i)chill', 'ì¹ ', regex=True)
    # í•œê¸€, ì˜ì–´ê°€ ì•„ë‹Œ ê²½ìš° ì²˜ë¦¬
    df['comment'] = df['comment']\
        .str.replace(r'[^a-zA-Z0-9ê°€-í£ã„±-ã…ã…-ã…£â™¡â™¥\!\?\@\#\$\%\^\&\*\(\)\-\_\=\+\\\~\,\.\/\<\>\[\]\{\}\;\:\'\"\s]', '', regex=True)
    # 2ê°œ ì´ìƒ ì—°ì†ëœ ë¬¸ì ì²˜ë¦¬
    df['comment'] = df['comment']\
        .str.replace(r'(.)\1{2,}', r'\1\1', regex=True)
    # ë¹ˆ ë¬¸ìì—´ì˜ ê²½ìš° empty ì²˜ë¦¬
    df['comment'] = df['comment'].str.strip()
    df['comment'] = df['comment'].fillna('[EMPTY]')

    return df

if __name__ == "__main__":
    # texts = [
    #     'í”¼ì‹ëŒ€í•™coï½ í•œë²ˆ ì¨ë´¤ëŠ”ë° ì§„ì§œ ê´œì°®ë”ë¼êµ¬ìš”. ì¹œêµ¬ë“¤í•œí…Œë„ ì¶”ì²œí–ˆì–´ìš”. ë‹¤ë“¤ í›„ê¸° í•œ ë²ˆ ë‚¨ê²¨ë´ìš”',
    #     'í•œìš©ì´í˜•ì²˜ëŸ¼ ê·¸ê±°ë§ìŒâˆÎ¿mì“°ë©´ ë¨¹ê±±ì •ì—†ì§€ á„á…©á†¼5 ê°œê¿€ë”±',
    #     'ê·¸ê±°ë§ìŒâ²¤â²Ÿm í•œë²ˆ ì¨ë´¤ëŠ”ë° ì§„ì§œ ê´œì°®ë”ë¼êµ¬ìš”. ì¹œêµ¬ë“¤í•œí…Œë„ ì¶”ì²œí–ˆì–´ìš”. ë‹¤ë“¤ í›„ê¸° í•œ ë²ˆ ë‚¨ê²¨ë´ìš”',
    #     'ìš°ì¼ì´í˜•ì´ ì•Œë ¤ì¤€ ìš°ì¼ì´í˜•â„­ê‹Ğ¼ ë•ì— ë“í…œ ì™„ë£Œ',
    #     'ê·¸ê±°ë§ìŒâ…­â²Ÿâ²˜ ë•ë¶„ì— ì»¤í”¼ê°’ í•´ê²°! á„Œá…³á†¨á„‰á…µ5á„†á…¡á†« ê°ì‚¬í•©ë‹ˆë‹¤',
    #     'ìš”ì¦˜ ì¢€ë¹„íŠ¸ë¦½âˆÎ¿m ê°™ì€ ì‚¬ì´íŠ¸ ë§ë˜ë°, ì—¬ê¸° ì§„ì§œ ì£¼ëŠ” ê±´ê°€ìš”? ê´œíˆ ì‹œê°„ ë‚­ë¹„ë ê¹Œë´ ê³ ë¯¼ ì¤‘ì´ì—ìš”',
    #     'ì•¼ ìŠ¤í¬í‹°ë¹„âˆâ²ŸÎœ ì¨ë´¤ëƒ? ë‚˜ á„‹á…©á„á…©á†¼ ë°›ì•„ì„œ ì¹˜í‚¨ ì‹œí‚´ ã…‹ã…‹ã…‹ ë ˆì „ë“œë„¤',
    #     'ì„ ìš±ì´í˜•ì´ ì¶”ì²œí•´ì¤€ ë³„ë†ˆë“¤Â¢ã€‡â„³ ì§„ì§œ ë¬¼ê±´ì´ë„¤ ã…‹ã…‹',
    #     'ê·œë‚¨ì´ëˆ„ë‚˜ê°€ ì¶”ì²œí•´ì¤€ ê·¸ê±°ë§ìŒâ„­0â²˜ ì§„ì§œ ë¬¼ê±´ì´ë„¤ ã…‹ã…‹',
    #     'ë‘ì¹œêµ¬CÎ¿â²˜ á„’á…ªá†«á„’á…¢á„Œá…®á„‚á…³á†«5á„á…©á†¼ê³ ë§™ë‹¤ ì˜¤í”ˆì¶•í•˜í•´ï¸',
    #     'ê·œì„ ì´í˜•ì€ ìœ ë³‘ì¬âˆÎ¿më§Œ ì“´ë‹¤í•˜ë”ë¼.. 5á„†á…¡á„‚á…¯á†«ì¦‰ê¸‰',
    #     'ìƒí˜‘ì´í˜• ë„ˆëœíŠ¸åŒšâµ”Ğ¼ ê·¸ë§Œ ì•Œë ¤ì¤˜ ã… ã…  ë‚˜ë§Œì•Œê³ ì‹¶ë‹¤ê³ ... ì§„ì§œ ì´ë²¤íŠ¸ã„·ã„·í•˜ë„¤',
    #     'ì•¼ ë‚˜ë„ ì¸í”¼ì‰°âˆâµ”Ğ¼ ë”°ë¼ í•´ë´¤ëŠ”ë°, ì§„ì§œ ë°”ë¡œ á„‹á…©á„†á…¡á†«á„‹á…¯á†«? ì´ê±° ì‹¤í™”ëƒ? ã…‹ã…‹ã…‹',
    #     'ìµœìŠ¹í•„ ì§„ì§œ ì˜í•œë‹¤ ë‘ì¹œêµ¬â…­ã€‡â²˜ ì˜ì“¸ê»˜  !!',
    #     'ìš°ì¼ì´í˜• ë•ì— ì˜¤ëŠ˜ íšŒì‹ë¹„ ë‚˜ì™”ë‹¤. ìš°ì¼ì´í˜•câ²Ÿâ„³ ì¸ì •ã…‹ã…‹ã…‹',
    #     'ìµœí™ì² Â¢0Îœì—ì„œ 5á„†á…¡á†«á„Œá…³á†¨á„‰á…µ ë°›ì€ í›„ê¸° ë´¤ëŠ”ë° ì†”ì§íˆ ë°˜ì‹ ë°˜ì˜í–ˆê±°ë“ ìš”. ê·¸ëŸ°ë° ì§„ì§œ ì£¼ë„¤ìš”. ì´ëŸ° ê±° ë˜ ìˆìœ¼ë©´ ê³µìœ  ë¶€íƒë“œë ¤ìš”!',
    #     '180ì´ˆí˜•ë“¤ì€ ìš°ì¼ì´í˜•COMë§Œ ì“´ë‹¤í•˜ë”ë¼.. á„‹á…©á„†á…¡á†«á„á…©á†¼ì¦‰ê¸‰',
    #     'ê·¸ê±°ë§ìŒCâµ”Îœì—ì„œ 5á„á…©á†¼ ë°›ì€ í›„ê¸° ë´¤ëŠ”ë° ì†”ì§íˆ ë°˜ì‹ ë°˜ì˜í–ˆê±°ë“ ìš”. ê·¸ëŸ°ë° ì§„ì§œ ì£¼ë„¤ìš”. ì´ëŸ° ê±° ë˜ ìˆìœ¼ë©´ ê³µìœ  ë¶€íƒë“œë ¤ìš”!',
    #     'ë³´ê²¸Câµ”Îœ? ì´ê±° ì¹œêµ¬ê°€ ì•Œë ¤ì¤¬ëŠ”ë° á„†á…®á„…á…§á„‹á…©á„†á…¡á†«á„‹á…¯á†« ë°”ë¡œ ì¤¬ëŒ€... ë‚˜ë„ í•´ë´ì•¼ê² ìŒ ã…‹ã…‹ã…‹',
    #     'COM',
    # ]

    # for text in texts:
    #     print(re.sub(r'', '', normalize_unicode_text(text)))

    with open('../tokens/emojis.txt', 'r', encoding='utf-8') as f:
        lines = [ line.strip() for line in f.readlines() ]
    df = pd.DataFrame(lines, columns=['comment'])

    _normalize_unicode(df)
    for emoji in df['comment']:
        print(emoji)

    # df = pd.read_csv('../model/dataset.csv', encoding='utf-8')
    # df['comment'] = df['comment'].map(lambda x: x.replace('\\', ',') if isinstance(x, str) else x)
    # df['comment'] = df['comment'].str.strip()

    # updated_logic_df = df.copy()

    # run_text_preprocessing(updated_logic_df, '../tokens/emojis.txt')

    # df['comment'] = df['comment'].map(lambda x: x.replace(',', '\\') if isinstance(x, str) else x)
    # updated_logic_df['comment'] = updated_logic_df['comment'].map(lambda x: x.replace(',', '\\') if isinstance(x, str) else x)

    # print(updated_logic_df.iloc[341])

    # comparison_updated_logic = df['comment'].compare(updated_logic_df['comment'])

    # special_tokens = [
    #     'DAYS,' 
    #     'DATE,' 
    #     'TIME',
    #     'FLOAT',
    #     'NUMBER',
    #     'DURATION',
    #     'RANGE',
    #     'COST',
    #     'RANK',
    #     'ANNIV',
    #     'MEASURE',
    #     'UNIT'
    # ]
    # pattern = r'(?:' + '|'.join(special_tokens) + r')'
    # mask = comparison_updated_logic.astype(str).apply(lambda x: x.str.contains(pattern)).any(axis=1)
    # filtered = comparison_updated_logic[mask]
    
    # with pd.ExcelWriter('comparition_results.xlsx') as writer:
    #     filtered.to_excel(writer, sheet_name="updated_logic")

    # from openpyxl import load_workbook
    # wb = load_workbook('comparition_results.xlsx')

    # ws_updated_logic = wb['updated_logic']

    # base_width = 30

    # for idx, column in enumerate(['B', 'C']):
    #     ws_updated_logic.column_dimensions[column].width = base_width * 6

    # # ìˆ˜ì •ëœ Excel íŒŒì¼ ì €ì¥
    # wb.save('comparition_results_with_custom_width.xlsx')